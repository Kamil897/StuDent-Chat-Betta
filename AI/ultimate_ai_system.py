"""
–ü–û–õ–ù–û–¶–ï–ù–ù–ê–Ø –°–ê–ú–û–†–ê–ó–í–ò–í–ê–Æ–©–ê–Ø–°–Ø –°–ò–°–¢–ï–ú–ê –ò–ò
–° –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π OpenAI GPT, –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏, –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º
–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
"""

import json
import os
import pickle
import random
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
try:
    from config import OPENAI_API_KEY, AI_CONFIG, NEURAL_NETWORK_CONFIG, VISUALIZATION_CONFIG
except ImportError:
    # –ï—Å–ª–∏ config.py –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
    AI_CONFIG = {
        'model': 'gpt-3.5-turbo',
        'temperature': 0.7,
        'max_tokens': 500,
        'learning_rate': 0.1,
        'improvement_interval': 10,
        'collective_agents': 3,
    }
    NEURAL_NETWORK_CONFIG = {
        'input_size': 50,
        'hidden_size': 100,
        'output_size': 20,
        'learning_rate': 0.01,
    }
    VISUALIZATION_CONFIG = {
        'save_plots': True,
        'plot_format': 'png',
        'update_interval': 5,
    }

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
    client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
except ImportError:
    OPENAI_AVAILABLE = False
    client = None
    print("‚ö†Ô∏è  OpenAI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")


# ==================== –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ ====================

class NeuralNetwork:
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, input_size: int = 50, hidden_size: int = 100, output_size: int = 20):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.W1 = np.random.randn(input_size, hidden_size) * 0.1
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.1
        self.b2 = np.zeros((1, output_size))
        
        self.training_history = []
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = np.tanh(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self._sigmoid(self.z2)
        return self.a2
    
    def _sigmoid(self, x: np.ndarray) -> np.ndarray:
        """–°–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        return self.forward(X)
    
    def train(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, epochs: int = 10):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è backpropagation)"""
        for epoch in range(epochs):
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            output = self.forward(X)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            error = y - output
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫)
            dW2 = np.dot(self.a1.T, error)
            db2 = np.sum(error, axis=0, keepdims=True)
            dW1 = np.dot(X.T, np.dot(error, self.W2.T) * (1 - np.power(self.a1, 2)))
            db1 = np.sum(np.dot(error, self.W2.T) * (1 - np.power(self.a1, 2)), axis=0, keepdims=True)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.W2 += learning_rate * dW2
            self.b2 += learning_rate * db2
            self.W1 += learning_rate * dW1
            self.b1 += learning_rate * db1
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            loss = np.mean(np.square(error))
            self.training_history.append(loss)
    
    def analyze_pattern(self, text: str, vectorizer) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        try:
            text_vector = vectorizer.transform([text]).toarray()
            prediction = self.predict(text_vector)
            return {
                'confidence': float(np.mean(prediction)),
                'pattern_score': float(np.max(prediction)),
                'complexity': float(np.std(prediction))
            }
        except:
            return {'confidence': 0.5, 'pattern_score': 0.5, 'complexity': 0.5}


# ==================== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° OPENAI ====================

class OpenAIIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, api_key: str, model: str = 'gpt-3.5-turbo'):
        self.api_key = api_key
        self.model = model
        self.client = OpenAI(api_key=api_key) if api_key and OPENAI_AVAILABLE else None
        self.usage_count = 0
        self.response_history = []
    
    def generate_response(self, query: str, context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT"""
        if not self.client:
            return "OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –∏ API –∫–ª—é—á."
        
        try:
            messages = [
                {"role": "system", "content": "–¢—ã —É–º–Ω—ã–π –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."}
            ]
            
            if context:
                messages.append({"role": "system", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}"})
            
            messages.append({"role": "user", "content": query})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=AI_CONFIG.get('temperature', 0.7),
                max_tokens=AI_CONFIG.get('max_tokens', 500)
            )
            
            answer = response.choices[0].message.content
            self.usage_count += 1
            self.response_history.append({
                'query': query,
                'response': answer,
                'timestamp': datetime.now().isoformat()
            })
            
            return answer
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {str(e)}"
    
    def generate_learning_task(self, knowledge_base_size: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
        if not self.client:
            return "–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–Ω–∞–Ω–∏–π"
        
        try:
            prompt = f"""–°–æ–∑–¥–∞–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –∑–∞–¥–∞—á—É –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –ò–ò. 
            –£—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç {knowledge_base_size} –∑–∞–ø–∏—Å–µ–π.
            –ó–∞–¥–∞—á–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤."""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=200
            )
            
            return response.choices[0].message.content
        except Exception as e:
            return f"–ó–∞–¥–∞—á–∞: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ {knowledge_base_size} –∑–∞–ø–∏—Å—è—Ö –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"


# ==================== –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –° –£–õ–£–ß–®–ï–ù–ò–Ø–ú–ò ====================

class AdvancedKnowledgeBase:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, storage_path: str = "ai_knowledge_base.json"):
        self.storage_path = storage_path
        self.knowledge: Dict[str, Any] = {}
        self.user_preferences: Dict[str, Any] = {}
        self.interaction_history: List[Dict[str, Any]] = []
        self.successful_responses: Dict[str, str] = {}
        self.vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
        self.text_vectors = []
        self.text_keys = []
        self.load()
    
    def load(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if os.path.exists(self.storage_path):
            try:
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.knowledge = data.get('knowledge', {})
                    self.user_preferences = data.get('user_preferences', {})
                    self.interaction_history = data.get('interaction_history', [])
                    self.successful_responses = data.get('successful_responses', {})
                    
                    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
                    if self.successful_responses:
                        texts = list(self.successful_responses.values())
                        if texts:
                            self.text_vectors = self.vectorizer.fit_transform(texts).toarray()
                            self.text_keys = list(self.successful_responses.keys())
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
    
    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            data = {
                'knowledge': self.knowledge,
                'user_preferences': self.user_preferences,
                'interaction_history': self.interaction_history[-1000:],
                'successful_responses': self.successful_responses,
                'last_updated': datetime.now().isoformat()
            }
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def add_knowledge(self, query: str, response: str, success: bool = True):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏—è —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        if success:
            query_lower = query.lower()
            self.successful_responses[query_lower] = response
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 1 —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏)
            if len(self.successful_responses) >= 1:
                texts = list(self.successful_responses.values())
                try:
                    self.text_vectors = self.vectorizer.fit_transform(texts).toarray()
                    self.text_keys = list(self.successful_responses.keys())
                except Exception as e:
                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º
                    self.text_vectors = []
                    self.text_keys = []
        
        interaction = {
            'query': query,
            'response': response,
            'success': success,
            'timestamp': datetime.now().isoformat()
        }
        self.interaction_history.append(interaction)
        self.save()
    
    def find_similar(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
        if isinstance(self.text_vectors, np.ndarray):
            if self.text_vectors.size == 0 or len(self.text_keys) == 0:
                return []
        else:
            if len(self.text_vectors) == 0 or len(self.text_keys) == 0:
                return []
        
        try:
            query_vector = self.vectorizer.transform([query.lower()]).toarray()
            similarities = cosine_similarity(query_vector, self.text_vectors)[0]
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            results = [(self.text_keys[i], float(similarities[i])) for i in top_indices if similarities[i] > 0.3]
            return results
        except:
            return []


# ==================== –ö–û–õ–õ–ï–ö–¢–ò–í–ù–´–ô –ò–ù–¢–ï–õ–õ–ï–ö–¢ ====================

class CollectiveIntelligence:
    """–°–∏—Å—Ç–µ–º–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ - –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç –≤–º–µ—Å—Ç–µ"""
    
    def __init__(self, num_agents: int = 3):
        self.num_agents = num_agents
        self.agents: List[Dict[str, Any]] = []
        self.communication_log: List[Dict[str, Any]] = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤
        for i in range(num_agents):
            self.agents.append({
                'id': i,
                'name': f"Agent_{i+1}",
                'specialization': self._assign_specialization(i),
                'knowledge_count': 0,
                'success_rate': 0.5,
                'responses': []
            })
    
    def _assign_specialization(self, agent_id: int) -> str:
        """–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç—É"""
        specializations = ['general', 'technical', 'creative', 'analytical', 'conversational']
        return specializations[agent_id % len(specializations)]
    
    def process_collectively(self, query: str, knowledge_base: AdvancedKnowledgeBase, 
                           openai_client: OpenAIIntegration) -> Tuple[str, Dict[str, Any]]:
        """–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        agent_responses = []
        agent_confidences = []
        
        # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å
        for agent in self.agents:
            # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            similar = knowledge_base.find_similar(query, top_k=1)
            if similar and similar[0][1] > 0.7:
                response = knowledge_base.successful_responses.get(similar[0][0], "")
                confidence = similar[0][1]
            else:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ GPT
                context = f"–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞: {agent['specialization']}"
                response = openai_client.generate_response(query, context)
                confidence = 0.6
            
            agent_responses.append(response)
            agent_confidences.append(confidence)
            agent['responses'].append({
                'query': query,
                'response': response,
                'confidence': confidence,
                'timestamp': datetime.now().isoformat()
            })
        
        # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        best_idx = np.argmax(agent_confidences)
        best_response = agent_responses[best_idx]
        best_agent = self.agents[best_idx]
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞
        best_agent['knowledge_count'] += 1
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.communication_log.append({
            'query': query,
            'agent_responses': agent_responses,
            'selected_agent': best_agent['name'],
            'avg_confidence': float(np.mean(agent_confidences)),
            'timestamp': datetime.now().isoformat()
        })
        
        return best_response, {
            'num_agents': self.num_agents,
            'selected_agent': best_agent['name'],
            'specialization': best_agent['specialization'],
            'avg_confidence': float(np.mean(agent_confidences)),
            'all_responses': agent_responses
        }


# ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================

class LearningVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self):
        self.performance_data: List[Dict[str, Any]] = []
        self.plot_dir = "learning_plots"
        os.makedirs(self.plot_dir, exist_ok=True)
    
    def add_data_point(self, interaction_count: int, accuracy: float, knowledge_size: int, 
                      improvement_rate: float):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.performance_data.append({
            'interaction': interaction_count,
            'accuracy': accuracy,
            'knowledge_size': knowledge_size,
            'improvement_rate': improvement_rate,
            'timestamp': datetime.now().isoformat()
        })
    
    def plot_learning_curve(self, save: bool = True):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è"""
        if len(self.performance_data) < 2:
            return
        
        interactions = [d['interaction'] for d in self.performance_data]
        accuracies = [d['accuracy'] for d in self.performance_data]
        knowledge_sizes = [d['knowledge_size'] for d in self.performance_data]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        ax1.plot(interactions, accuracies, 'b-', marker='o', label='–¢–æ—á–Ω–æ—Å—Ç—å')
        ax1.set_xlabel('–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è')
        ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        ax1.set_title('–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è - –¢–æ—á–Ω–æ—Å—Ç—å')
        ax1.grid(True)
        ax1.legend()
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞–∑–º–µ—Ä–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        ax2.plot(interactions, knowledge_sizes, 'g-', marker='s', label='–†–∞–∑–º–µ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
        ax2.set_xlabel('–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è')
        ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')
        ax2.set_title('–†–æ—Å—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
        ax2.grid(True)
        ax2.legend()
        
        plt.tight_layout()
        
        if save:
            filename = os.path.join(self.plot_dir, f"learning_curve_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            plt.savefig(filename)
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
        
        plt.close()
    
    def plot_agent_performance(self, collective: CollectiveIntelligence, save: bool = True):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤"""
        agent_names = [a['name'] for a in collective.agents]
        knowledge_counts = [a['knowledge_count'] for a in collective.agents]
        success_rates = [a['success_rate'] for a in collective.agents]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–Ω–∞–Ω–∏–π
        ax1.bar(agent_names, knowledge_counts, color='skyblue')
        ax1.set_xlabel('–ê–≥–µ–Ω—Ç—ã')
        ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–Ω–∏–π')
        ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏')
        ax1.tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        ax2.bar(agent_names, success_rates, color='lightgreen')
        ax2.set_xlabel('–ê–≥–µ–Ω—Ç—ã')
        ax2.set_ylabel('–£—Å–ø–µ—à–Ω–æ—Å—Ç—å')
        ax2.set_title('–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save:
            filename = os.path.join(self.plot_dir, f"agent_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            plt.savefig(filename)
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –∞–≥–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
        
        plt.close()


# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ó–ê–î–ê–ß –î–õ–Ø –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–Ø ====================

class SelfLearningTaskGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–¥–∞—á –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, openai_client: OpenAIIntegration):
        self.openai_client = openai_client
        self.generated_tasks: List[Dict[str, Any]] = []
    
    def generate_task(self, knowledge_base_size: int, weak_areas: List[str] = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
        task_description = self.openai_client.generate_learning_task(knowledge_base_size)
        
        task = {
            'description': task_description,
            'type': self._determine_task_type(task_description),
            'difficulty': self._estimate_difficulty(knowledge_base_size),
            'timestamp': datetime.now().isoformat(),
            'completed': False
        }
        
        self.generated_tasks.append(task)
        return task
    
    def _determine_task_type(self, description: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        description_lower = description.lower()
        if '–∞–Ω–∞–ª–∏–∑' in description_lower or '–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å' in description_lower:
            return 'analysis'
        elif '–≥–µ–Ω–µ—Ä–∞—Ü–∏—è' in description_lower or '—Å–æ–∑–¥–∞—Ç—å' in description_lower:
            return 'generation'
        elif '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è' in description_lower or '—É–ª—É—á—à–∏—Ç—å' in description_lower:
            return 'optimization'
        else:
            return 'general'
    
    def _estimate_difficulty(self, knowledge_base_size: int) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏"""
        if knowledge_base_size < 10:
            return 'easy'
        elif knowledge_base_size < 50:
            return 'medium'
        else:
            return 'hard'


# ==================== –ì–õ–ê–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê ====================

class UltimateAISystem:
    """–ì–ª–∞–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ—Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ—Å—è –ò–ò —Å–æ –≤—Å–µ–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏"""
    
    def __init__(self):
        self.knowledge_base = AdvancedKnowledgeBase()
        self.openai_client = OpenAIIntegration(OPENAI_API_KEY, AI_CONFIG.get('model', 'gpt-3.5-turbo'))
        self.neural_network = NeuralNetwork(
            NEURAL_NETWORK_CONFIG['input_size'],
            NEURAL_NETWORK_CONFIG['hidden_size'],
            NEURAL_NETWORK_CONFIG['output_size']
        )
        self.collective = CollectiveIntelligence(AI_CONFIG.get('collective_agents', 3))
        self.visualizer = LearningVisualizer()
        self.task_generator = SelfLearningTaskGenerator(self.openai_client)
        
        self.interaction_count = 0
        self.improvement_count = 0
        self.accuracy_history: List[float] = []
        
        print("ü§ñ –ü–û–õ–ù–û–¶–ï–ù–ù–ê–Ø –°–ê–ú–û–†–ê–ó–í–ò–í–ê–Æ–©–ê–Ø–°–Ø –°–ò–°–¢–ï–ú–ê –ò–ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ê")
        print(f"üìä –ê–≥–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–µ: {self.collective.num_agents}")
        print(f"üß† –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å: {self.neural_network.input_size} -> {self.neural_network.hidden_size} -> {self.neural_network.output_size}")
        if OPENAI_AVAILABLE and OPENAI_API_KEY:
            print("‚úÖ OpenAI GPT –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω")
        else:
            print("‚ö†Ô∏è  OpenAI –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def process_query(self, query: str, use_collective: bool = True) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º"""
        self.interaction_count += 1
        start_time = time.time()
        
        # –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        if use_collective:
            response, collective_info = self.collective.process_collectively(
                query, self.knowledge_base, self.openai_client
            )
        else:
            # –û–¥–∏–Ω–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            similar = self.knowledge_base.find_similar(query, top_k=1)
            if similar and similar[0][1] > 0.7:
                response = self.knowledge_base.successful_responses.get(similar[0][0], "")
            else:
                response = self.openai_client.generate_response(query)
            collective_info = {}
        
        execution_time = time.time() - start_time
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é
        pattern_analysis = self.neural_network.analyze_pattern(
            query, self.knowledge_base.vectorizer
        )
        
        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success = pattern_analysis['confidence'] > 0.5
        accuracy = pattern_analysis['confidence']
        self.accuracy_history.append(accuracy)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        self.knowledge_base.add_knowledge(query, response, success)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if self.interaction_count % VISUALIZATION_CONFIG.get('update_interval', 5) == 0:
            avg_accuracy = np.mean(self.accuracy_history[-10:]) if self.accuracy_history else 0.5
            self.visualizer.add_data_point(
                self.interaction_count,
                avg_accuracy,
                len(self.knowledge_base.successful_responses),
                self.improvement_count / max(1, self.interaction_count)
            )
            self.visualizer.plot_learning_curve()
            if use_collective:
                self.visualizer.plot_agent_performance(self.collective)
        
        # –°–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ
        if self.interaction_count % AI_CONFIG.get('improvement_interval', 10) == 0:
            self._self_improve()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
        if self.interaction_count % 20 == 0:
            task = self.task_generator.generate_task(len(self.knowledge_base.successful_responses))
            print(f"\nüìö –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–¥–∞—á–∞ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è: {task['description']}")
        
        return {
            'response': response,
            'execution_time': execution_time,
            'pattern_analysis': pattern_analysis,
            'collective_info': collective_info,
            'accuracy': accuracy,
            'interaction_count': self.interaction_count
        }
    
    def _self_improve(self):
        """–ü—Ä–æ—Ü–µ—Å—Å —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è"""
        self.improvement_count += 1
        print(f"\nüîÑ [–°–ê–ú–û–£–õ–£–ß–®–ï–ù–ò–ï #{self.improvement_count}]")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(self.accuracy_history) > 10:
            recent_accuracy = np.mean(self.accuracy_history[-10:])
            print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10): {recent_accuracy:.3f}")
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
            if recent_accuracy < 0.7:
                print("üß† –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
        
        print(f"üìà –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.interaction_count}")
        print(f"üíæ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {len(self.knowledge_base.successful_responses)} –∑–∞–ø–∏—Å–µ–π")
        print(f"ü§ù –ê–≥–µ–Ω—Ç–æ–≤: {self.collective.num_agents}")
        print("‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n")
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'interactions': self.interaction_count,
            'improvements': self.improvement_count,
            'knowledge_base_size': len(self.knowledge_base.successful_responses),
            'avg_accuracy': np.mean(self.accuracy_history[-10:]) if self.accuracy_history else 0.0,
            'openai_usage': self.openai_client.usage_count,
            'collective_agents': self.collective.num_agents,
            'neural_network_trained': len(self.neural_network.training_history) > 0
        }


# ==================== –ò–ù–¢–ï–†–§–ï–ô–° ====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("="*70)
    print(" " * 15 + "–ü–û–õ–ù–û–¶–ï–ù–ù–ê–Ø –°–ê–ú–û–†–ê–ó–í–ò–í–ê–Æ–©–ê–Ø–°–Ø –°–ò–°–¢–ï–ú–ê –ò–ò")
    print("="*70)
    print("\n–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI GPT")
    print("‚úÖ –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç")
    print("‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á")
    print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è")
    print("="*70 + "\n")
    
    system = UltimateAISystem()
    
    print("\n–í–≤–µ–¥–∏—Ç–µ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞, 'status' –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞\n")
    
    while True:
        query = input("–í—ã: ").strip()
        
        if query.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
            print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤—Å–µ –¥–∞–Ω–Ω—ã–µ...")
            system.knowledge_base.save()
            print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ! –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if query.lower() == 'status':
            status = system.get_status()
            print("\n" + "="*70)
            print("–°–¢–ê–¢–£–° –°–ò–°–¢–ï–ú–´")
            print("="*70)
            for key, value in status.items():
                print(f"{key}: {value}")
            print("="*70 + "\n")
            continue
        
        if not query:
            continue
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        result = system.process_query(query, use_collective=True)
        
        print(f"\nü§ñ –ò–ò: {result['response']}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {result['execution_time']:.2f}—Å")
        if result.get('collective_info'):
            print(f"üë• –ê–≥–µ–Ω—Ç: {result['collective_info'].get('selected_agent', 'N/A')}")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['accuracy']:.2%}\n")


if __name__ == "__main__":
    main()

